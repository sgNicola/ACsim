{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'05': ['05_aid.csv', '05_rcd.csv', '05_hira.csv', '05_dvca.csv'], '08': ['08_rcd.csv', '08_aid.csv', '08_hira.csv', '08_dvca.csv'], '07': ['07_hira.csv', '07_aid.csv', '07_rcd.csv', '07_dvca.csv'], '03': ['03_rcd.csv', '03_hira.csv', '03_aid.csv', '03_dvca.csv'], '01': ['01_rcd.csv', '01_hira.csv', '01_aid.csv', '01_dvca.csv'], '12': ['12_dvca.csv', '12_hira.csv', '12_aid.csv', '12_rcd.csv'], '04': ['04_aid.csv', '04_dvca.csv', '04_rcd.csv', '04_hira.csv'], '09': ['09_rcd.csv', '09_dvca.csv', '09_aid.csv', '09_hira.csv'], '06': ['06_dvca.csv', '06_rcd.csv', '06_aid.csv', '06_hira.csv'], '11': ['11_dvca.csv', '11_aid.csv', '11_rcd.csv', '11_hira.csv'], '02': ['02_rcd.csv', '02_aid.csv', '02_dvca.csv', '02_hira.csv'], '10': ['10_rcd.csv', '10_aid.csv', '10_dvca.csv', '10_hira.csv']} {'10': ['10_groundtruth.csv'], '08': ['08_groundtruth.csv'], '12': ['12_groundtruth.csv'], '09': ['09_groundtruth.csv'], '04': ['04_groundtruth.csv'], '02': ['02_groundtruth.csv'], '11': ['11_groundtruth.csv'], '05': ['05_groundtruth.csv'], '03': ['03_groundtruth.csv'], '01': ['01_groundtruth.csv'], '07': ['07_groundtruth.csv'], '06': ['06_groundtruth.csv']}\n"
     ]
    }
   ],
   "source": [
    "print(method_files,groundtruth_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 91\n",
      "False Positives (FP): 9\n",
      "False Negatives (FN): 109\n",
      "Precision: 91.0 %\n",
      "Recall: 45.5 %\n",
      "F1 Score: 60.67 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "def flatten_nested_list(x):\n",
    "    # Check if the first element is a list and it's not empty\n",
    "    if x and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_and_process_data(file_id, method, directory=\"/home/anonymous/ACsim/results/inject_issues\"):\n",
    "    \"\"\"\n",
    "    Load ground truth and prediction data from specified files, and split their contents for further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - file_id: str, identifier for the specific result files\n",
    "    - method: str, methodology applied in the prediction file\n",
    "    - directory: str, base directory where the files are stored\n",
    "\n",
    "    Returns:\n",
    "    - true_labels_lists: list, list of lists containing the ground truth labels\n",
    "    - pred_labels_lists: list, list of lists containing the predicted labels\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    groundtruth_file = f\"{file_id}_groundtruth.csv\"\n",
    "    prediction_file = f\"{file_id}_{method}.csv\"\n",
    "    groundtruth_path = os.path.join(directory, groundtruth_file)\n",
    "    prediction_path = os.path.join(directory, prediction_file)\n",
    "\n",
    "    # Load data from CSV files\n",
    "    data = pd.read_csv(groundtruth_path)\n",
    "    predictions = pd.read_csv(prediction_path)\n",
    "\n",
    "    # Process the 'groundtruth_1' and 'Roots' columns by splitting each string by tabs\n",
    "    true_labels_lists = data['groundtruth_1'].apply(lambda x: x.split('\\t'))\n",
    "    pred_labels_lists = predictions['Roots'].apply(lambda x: x.split('\\t'))\n",
    "    # pred_labels_lists = predictions['Roots'].apply(flatten_nested_list)\n",
    "    # true_labels_lists = data['groundtruth_1'].apply(flatten_nested_list)\n",
    "    \n",
    "    return true_labels_lists, pred_labels_lists\n",
    "\n",
    "def calculate_metrics(true_labels_lists, pred_labels_lists):\n",
    "    \"\"\"\n",
    "    Calculate the True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    from lists of true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels_lists: list of lists, where each sublist contains true labels for a sample\n",
    "    - pred_labels_lists: list of lists, where each sublist contains predicted labels for a sample\n",
    "\n",
    "    Returns:\n",
    "    - TP: int, number of True Positives\n",
    "    - FP: int, number of False Positives\n",
    "    - FN: int, number of False Negatives\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    # Iterate over pairs of true and predicted labels\n",
    "    for true_labels, pred_labels in zip(true_labels_lists, pred_labels_lists):\n",
    "        # pred_label_list = ast.literal_eval(pred_labels) \n",
    "        # true_label_list = ast.literal_eval(true_labels)\n",
    "        true_label_set = set(true_labels)\n",
    "        # true_label_set = set(true_label_list)\n",
    "        pred_label_set = set(pred_labels)\n",
    "        # pred_label_set = set(pred_label_list)\n",
    "        # Calculate metrics\n",
    "        TP += len(true_label_set & pred_label_set)  # Intersection gives the TP\n",
    "        FP += len(pred_label_set - true_label_set)  # Predicted labels not in true labels\n",
    "        FN += len(true_label_set - pred_label_set)  # True labels not in predicted labels\n",
    "\n",
    "    return TP, FP, FN\n",
    "\n",
    "def calculate_evaluation(TP, FP, FN):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        TP (int): Number of True Positives\n",
    "        FP (int): Number of False Positives\n",
    "        FN (int): Number of False Negatives\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score.\n",
    "        recall (float): The recall score.\n",
    "        f1_score (float): The F1 score.\n",
    "    \"\"\"\n",
    "    # Calculate precision\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    if TP + FN > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    # Calculate F1-score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "        # Round and convert to percentage\n",
    "    precision = round(precision * 100, 2)\n",
    "    recall = round(recall * 100, 2)\n",
    "    f1_score = round(f1_score * 100, 2)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "# Example usage:\n",
    "file_id = \"14\"\n",
    "method = \"dvca\"\n",
    "true_labels, pred_labels = load_and_process_data(file_id, method)\n",
    "# Example usage, assuming true_labels_lists and pred_labels_lists are already defined\n",
    "TP, FP, FN = calculate_metrics(true_labels, pred_labels)\n",
    "precision, recall, f1_score = calculate_evaluation(TP, FP, FN)\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(\"Precision:\", precision, \"%\")\n",
    "print(\"Recall:\", recall, \"%\")\n",
    "print(\"F1 Score:\", f1_score, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['groundtruth_1', 'groundtruth_2'], dtype='object')\n",
      "0     ['euclidean_cluster']\n",
      "1     ['euclidean_cluster']\n",
      "2     ['euclidean_cluster']\n",
      "3     ['euclidean_cluster']\n",
      "4     ['euclidean_cluster']\n",
      "              ...          \n",
      "95    ['euclidean_cluster']\n",
      "96    ['euclidean_cluster']\n",
      "97    ['euclidean_cluster']\n",
      "98    ['euclidean_cluster']\n",
      "99    ['euclidean_cluster']\n",
      "Name: groundtruth_1, Length: 100, dtype: object 0             ['object_association_merger_1']\n",
      "1     ['obstacle_pointcloud_based_validator']\n",
      "2     ['obstacle_pointcloud_based_validator']\n",
      "3             ['object_association_merger_1']\n",
      "4             ['object_association_merger_1']\n",
      "                       ...                   \n",
      "95            ['object_association_merger_1']\n",
      "96            ['object_association_merger_1']\n",
      "97    ['obstacle_pointcloud_based_validator']\n",
      "98            ['clustering_shape_estimation']\n",
      "99                      ['euclidean_cluster']\n",
      "Name: Roots, Length: 100, dtype: object\n",
      "True Positives (TP): 48\n",
      "False Positives (FP): 52\n",
      "False Negatives (FN): 52\n",
      "Precision: 48.0 %\n",
      "Recall: 48.0 %\n",
      "F1 Score: 48.0 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "def flatten_nested_list(x):\n",
    "    # Check if the first element is a list and it's not empty\n",
    "    if x and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_and_process_data(file_id, method, directory=\"/home/anonymous/ACsim/results/github_issues\"):\n",
    "    \"\"\"\n",
    "    Load ground truth and prediction data from specified files, and split their contents for further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - file_id: str, identifier for the specific result files\n",
    "    - method: str, methodology applied in the prediction file\n",
    "    - directory: str, base directory where the files are stored\n",
    "\n",
    "    Returns:\n",
    "    - true_labels_lists: list, list of lists containing the ground truth labels\n",
    "    - pred_labels_lists: list, list of lists containing the predicted labels\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    groundtruth_file = f\"{file_id}_groundtruth.csv\"\n",
    "    prediction_file = f\"{file_id}_{method}.csv\"\n",
    "    groundtruth_path = os.path.join(directory, groundtruth_file)\n",
    "    prediction_path = os.path.join(directory, prediction_file)\n",
    "\n",
    "    # Load data from CSV files\n",
    "    data = pd.read_csv(groundtruth_path)\n",
    "    print(data.columns)\n",
    "    data['groundtruth_2'] = data['groundtruth_2'].fillna('') \n",
    "    predictions = pd.read_csv(prediction_path)\n",
    "    predictions['Roots'] = predictions['Roots'].fillna('') \n",
    "    \n",
    "    # Process the 'groundtruth_1' and 'Roots' columns by splitting each string by tabs\n",
    "    # true_labels_lists = data['groundtruth_1'].apply(lambda x: x.split('\\t'))\n",
    "    \n",
    "    # pred_labels_lists = predictions['Roots'].apply(lambda x: x.split('\\t'))\n",
    "    true_labels_lists = data['groundtruth_1'].apply(flatten_nested_list)\n",
    "    pred_labels_lists = predictions['Roots'].apply(flatten_nested_list)\n",
    "    \n",
    "    return true_labels_lists, pred_labels_lists\n",
    "\n",
    "def calculate_metrics(true_labels_lists, pred_labels_lists):\n",
    "    \"\"\"\n",
    "    Calculate the True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    from lists of true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels_lists: list of lists, where each sublist contains true labels for a sample\n",
    "    - pred_labels_lists: list of lists, where each sublist contains predicted labels for a sample\n",
    "\n",
    "    Returns:\n",
    "    - TP: int, number of True Positives\n",
    "    - FP: int, number of False Positives\n",
    "    - FN: int, number of False Negatives\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    # Iterate over pairs of true and predicted labels\n",
    "    for true_labels, pred_labels in zip(true_labels_lists, pred_labels_lists):\n",
    "        pred_label_list = ast.literal_eval(pred_labels)\n",
    "        true_label_list = ast.literal_eval(true_labels)\n",
    "        true_label_set = set(true_label_list)\n",
    "        pred_label_set = set(pred_label_list)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        TP += len(true_label_set & pred_label_set)  # Intersection gives the TP\n",
    "        FP += len(pred_label_set - true_label_set)  # Predicted labels not in true labels\n",
    "        FN += len(true_label_set - pred_label_set)  # True labels not in predicted labels\n",
    "\n",
    "    return TP, FP, FN\n",
    "\n",
    "def calculate_evaluation(TP, FP, FN):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        TP (int): Number of True Positives\n",
    "        FP (int): Number of False Positives\n",
    "        FN (int): Number of False Negatives\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score.\n",
    "        recall (float): The recall score.\n",
    "        f1_score (float): The F1 score.\n",
    "    \"\"\"\n",
    "    # Calculate precision\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    if TP + FN > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    # Calculate F1-score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "        # Round and convert to percentage\n",
    "    precision = round(precision * 100, 2)\n",
    "    recall = round(recall * 100, 2)\n",
    "    f1_score = round(f1_score * 100, 2)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "# Example usage:\n",
    "file_id = \"02\"\n",
    "method = \"dvca\"\n",
    "true_labels, pred_labels = load_and_process_data(file_id, method)\n",
    "print(true_labels,pred_labels)\n",
    "# Example usage, assuming true_labels_lists and pred_labels_lists are already defined\n",
    "TP, FP, FN = calculate_metrics(true_labels, pred_labels)\n",
    "precision, recall, f1_score = calculate_evaluation(TP, FP, FN)\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(\"Precision:\", precision, \"%\")\n",
    "print(\"Recall:\", recall, \"%\")\n",
    "print(\"F1 Score:\", f1_score, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives (TP): 56\n",
      "False Positives (FP): 44\n",
      "False Negatives (FN): 144\n",
      "Precision: 56.0 %\n",
      "Recall: 28.0 %\n",
      "F1 Score: 37.33 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "def flatten_nested_list(x):\n",
    "    # Check if the first element is a list and it's not empty\n",
    "    if x and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "def load_and_process_data(file_id, method, directory=\"/home/anonymous/ACsim/results/github_issues\"):\n",
    "    \"\"\"\n",
    "    Load ground truth and prediction data from specified files, and calculate the union of labels from groundtruth columns.\n",
    "\n",
    "    Parameters:\n",
    "    - file_id: str, identifier for the specific result files\n",
    "    - method: str, methodology applied in the prediction file\n",
    "    - directory: str, base directory where the files are stored\n",
    "\n",
    "    Returns:\n",
    "    - true_labels_union: list, list of lists containing the union of ground truth labels from multiple columns\n",
    "    - pred_labels_lists: list, list of lists containing the predicted labels\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    groundtruth_file = f\"{file_id}_groundtruth.csv\"\n",
    "    prediction_file = f\"{file_id}_{method}.csv\"\n",
    "    groundtruth_path = os.path.join(directory, groundtruth_file)\n",
    "    prediction_path = os.path.join(directory, prediction_file)\n",
    "\n",
    "    # Load data from CSV files\n",
    "    data = pd.read_csv(groundtruth_path)\n",
    "    predictions = pd.read_csv(prediction_path)\n",
    "\n",
    "    # Ensure columns are filled with empty lists if NaN and evaluate strings to lists\n",
    "    # for col in ['groundtruth_1', 'groundtruth_2', 'groundtruth_3']:\n",
    "    for col in ['groundtruth_1', 'groundtruth_2']:\n",
    "        data[col] = data[col].fillna('[]').apply(eval)\n",
    "\n",
    "    # Calculate the union of the groundtruth columns\n",
    "    def union_lists(row):\n",
    "        # return list(set(row['groundtruth_1']) | set(row['groundtruth_2']) |set(row['groundtruth_3']))\n",
    "        return list(set(row['groundtruth_1']) | set(row['groundtruth_2']))\n",
    "\n",
    "    true_labels_union = data.apply(union_lists, axis=1)     # Process prediction labels if necessary\n",
    "    predictions['Roots'] = predictions['Roots'].fillna('[]').apply(eval)\n",
    "    pred_labels_lists = predictions['Roots'].apply(flatten_nested_list)\n",
    "    return true_labels_union, pred_labels_lists\n",
    "\n",
    "def calculate_metrics(true_labels_lists, pred_labels_lists):\n",
    "    \"\"\"\n",
    "    Calculate the True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    from lists of true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels_lists: list of lists, where each sublist contains true labels for a sample\n",
    "    - pred_labels_lists: list of lists, where each sublist contains predicted labels for a sample\n",
    "\n",
    "    Returns:\n",
    "    - TP: int, number of True Positives\n",
    "    - FP: int, number of False Positives\n",
    "    - FN: int, number of False Negatives\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    # Iterate over pairs of true and predicted labels\n",
    "    for true_labels, pred_labels in zip(true_labels_lists, pred_labels_lists):\n",
    "        # pred_label_list = ast.literal_eval(pred_labels)\n",
    "        # pred_label_set = set(pred_label_list)\n",
    "        # true_label_list = ast.literal_eval(true_labels)\n",
    "        # true_label_set = set(true_label_list)\n",
    "        pred_label_set = set(pred_labels)\n",
    "        true_label_set = set(true_labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        TP += len(true_label_set & pred_label_set)  # Intersection gives the TP\n",
    "        FP += len(pred_label_set - true_label_set)  # Predicted labels not in true labels\n",
    "        FN += len(true_label_set - pred_label_set)  # True labels not in predicted labels\n",
    "\n",
    "    return TP, FP, FN\n",
    "\n",
    "def calculate_evaluation(TP, FP, FN):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        TP (int): Number of True Positives\n",
    "        FP (int): Number of False Positives\n",
    "        FN (int): Number of False Negatives\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score.\n",
    "        recall (float): The recall score.\n",
    "        f1_score (float): The F1 score.\n",
    "    \"\"\"\n",
    "    # Calculate precision\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    if TP + FN > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    # Calculate F1-score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "        # Round and convert to percentage\n",
    "    precision = round(precision * 100, 2)\n",
    "    recall = round(recall * 100, 2)\n",
    "    f1_score = round(f1_score * 100, 2)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "# Example usage:\n",
    "file_id = \"02\"\n",
    "method = \"dvca\"\n",
    "true_labels, pred_labels = load_and_process_data(file_id, method)\n",
    "# Example usage, assuming true_labels_lists and pred_labels_lists are already defined\n",
    "TP, FP, FN = calculate_metrics(true_labels, pred_labels)\n",
    "precision, recall, f1_score = calculate_evaluation(TP, FP, FN)\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(\"Precision:\", precision, \"%\")\n",
    "print(\"Recall:\", recall, \"%\")\n",
    "print(\"F1 Score:\", f1_score, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['experiment_id', 'id', 'Roots'], dtype='object')\n",
      "True Positives (TP): 87\n",
      "False Positives (FP): 13\n",
      "False Negatives (FN): 13\n",
      "Precision: 87.0 %\n",
      "Recall: 87.0 %\n",
      "F1 Score: 87.0 %\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "def flatten_nested_list(x):\n",
    "    # Check if the first element is a list and it's not empty\n",
    "    if x and isinstance(x[0], list):\n",
    "        return x[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_and_process_data(file_id, method, directory=\"/home/anonymous/ACsim/results/github_issues\"):\n",
    "    \"\"\"\n",
    "    Load ground truth and prediction data from specified files, and split their contents for further processing.\n",
    "\n",
    "    Parameters:\n",
    "    - file_id: str, identifier for the specific result files\n",
    "    - method: str, methodology applied in the prediction file\n",
    "    - directory: str, base directory where the files are stored\n",
    "\n",
    "    Returns:\n",
    "    - true_labels_lists: list, list of lists containing the ground truth labels\n",
    "    - pred_labels_lists: list, list of lists containing the predicted labels\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    groundtruth_file = f\"{file_id}_groundtruth.csv\"\n",
    "    prediction_file = f\"{file_id}_{method}.csv\"\n",
    "    groundtruth_path = os.path.join(directory, groundtruth_file)\n",
    "    prediction_path = os.path.join(directory, prediction_file)\n",
    "\n",
    "    # Load data from CSV files\n",
    "    data = pd.read_csv(groundtruth_path)\n",
    "    predictions = pd.read_csv(prediction_path)\n",
    "    print(predictions.columns)\n",
    "    predictions['Roots'] = predictions['Roots'].fillna('') \n",
    "    # Process the 'groundtruth_1' and 'Roots' columns by splitting each string by tabs\n",
    "    true_labels_lists_1 = data['groundtruth_1'].apply(flatten_nested_list)\n",
    "    true_labels_lists_2 = data['groundtruth_2'].apply(flatten_nested_list)\n",
    "    # true_labels_lists_3 = data['groundtruth_3'].apply(flatten_nested_list)\n",
    "    pred_labels_lists = predictions['Roots'].apply(flatten_nested_list)\n",
    "    \n",
    "    return true_labels_lists_1, true_labels_lists_2,pred_labels_lists\n",
    "\n",
    "def calculate_metrics(true_labels_lists_1,true_labels_lists_2,  pred_labels_lists):\n",
    "    \"\"\"\n",
    "    Calculate the True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "    from lists of true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels_lists: list of lists, where each sublist contains true labels for a sample\n",
    "    - pred_labels_lists: list of lists, where each sublist contains predicted labels for a sample\n",
    "\n",
    "    Returns:\n",
    "    - TP: int, number of True Positives\n",
    "    - FP: int, number of False Positives\n",
    "    - FN: int, number of False Negatives\n",
    "    \"\"\"\n",
    "    TP = FP = FN = 0\n",
    "\n",
    "    # Iterate over pairs of true and predicted labels\n",
    "    for true_labels_1, true_labels_2, pred_labels in zip(true_labels_lists_1,true_labels_lists_2, pred_labels_lists):\n",
    "        pred_label_list = ast.literal_eval(pred_labels)\n",
    "        pred_label_set = set(pred_label_list)\n",
    "        true_label_list_1 = ast.literal_eval(true_labels_1)\n",
    "        true_label_set_1 = set(true_label_list_1)\n",
    "        true_label_list_2 = ast.literal_eval(true_labels_2)\n",
    "        true_label_set_2 = set(true_label_list_2)\n",
    "        # true_label_list_3 = ast.literal_eval(true_labels_3)\n",
    "        # true_label_set_3 = set(true_label_list_3)\n",
    "         \n",
    "        selected_true_label_set = true_label_set_1 if len(true_label_set_1 & pred_label_set) > len(true_label_set_2 & pred_label_set) else true_label_set_2\n",
    "        TP += len( selected_true_label_set & pred_label_set)  # Intersection with either true label set gives the TP\n",
    "        FP += len(pred_label_set -selected_true_label_set)  # Predicted labels not in either true label set\n",
    "        FN += len(selected_true_label_set - pred_label_set)\n",
    "\n",
    "        #FN += len((true_label_set_1 - pred_label_set)&(true_label_set_2 - pred_label_set)) # True labels not in predicted labels\n",
    "        \n",
    "    return TP, FP, FN\n",
    "\n",
    "def calculate_evaluation(TP, FP, FN):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        TP (int): Number of True Positives\n",
    "        FP (int): Number of False Positives\n",
    "        FN (int): Number of False Negatives\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score.\n",
    "        recall (float): The recall score.\n",
    "        f1_score (float): The F1 score.\n",
    "    \"\"\"\n",
    "    # Calculate precision\n",
    "    if TP + FP > 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    if TP + FN > 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    # Calculate F1-score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0\n",
    "\n",
    "        # Round and convert to percentage\n",
    "    precision = round(precision * 100, 2)\n",
    "    recall = round(recall * 100, 2)\n",
    "    f1_score = round(f1_score * 100, 2)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "# Example usage:\n",
    "file_id = \"02\"\n",
    "method = \"aid\"\n",
    "true_labels_1,true_labels_2,  pred_labels = load_and_process_data(file_id, method)\n",
    "# Example usage, assuming true_labels_lists and pred_labels_lists are already defined\n",
    "TP, FP, FN = calculate_metrics(true_labels_1, true_labels_2, pred_labels)\n",
    "precision, recall, f1_score = calculate_evaluation(TP, FP, FN)\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(\"Precision:\", precision, \"%\")\n",
    "print(\"Recall:\", recall, \"%\")\n",
    "print(\"F1 Score:\", f1_score, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
